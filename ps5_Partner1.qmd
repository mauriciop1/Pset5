---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL of the HHS OIG Enforcement Actions page
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send a GET request to fetch the page content
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')  # Using the default parser

# Lists to hold data
titles, dates, categories, links = [], [], [], []

# Find enforcement action entries
for item in soup.select('h2.usa-card__heading'):
    # Extract title
    title = item.select_one('a').text.strip()
    
    # Extract and complete the link
    link = "https://oig.hhs.gov" + item.select_one('a')['href']
    
    # Extract category
    category = item.find_next('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1').text.strip()
    
    # Extract date
    date = item.find_next('span', class_='text-base-dark padding-right-105').text.strip()
    
    # Append to lists
    titles.append(title)
    links.append(link)
    categories.append(category)
    dates.append(date)

# Create a DataFrame
df = pd.DataFrame({
    "Title": titles,
    "Date": dates,
    "Category": categories,
    "Link": links
})

# Show the first few rows
print(df.head())

```

  
### 2. Crawling (PARTNER 1)

```{python}

import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL of the HHS OIG Enforcement Actions page
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send a GET request to fetch the page content
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')  # Use 'html.parser' instead of 'lxml'

# Lists to hold data
titles, dates, categories, links, agencies = [], [], [], [], []

# Find enforcement action entries
for item in soup.select('h2.usa-card__heading'):
    # Extract title
    title = item.select_one('a').text.strip()
    
    # Extract and complete the link
    link = "https://oig.hhs.gov" + item.select_one('a')['href']
    
    # Extract category
    category = item.find_next('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1').text.strip()
    
    # Extract date
    date = item.find_next('span', class_='text-base-dark padding-right-105').text.strip()
    
    # Visit the link to extract the agency
    enforcement_page = requests.get(link)
    enforcement_soup = BeautifulSoup(enforcement_page.text, 'html.parser')
    
    # Extract agency name (based on the example, you may need to adjust the selector)
    agency_tag = enforcement_soup.select('div.content p:nth-of-type(3)')
    agency_name = agency_tag[0].text.strip() if agency_tag else "Unknown"
    
    # Append to lists
    titles.append(title)
    links.append(link)
    categories.append(category)
    dates.append(date)
    agencies.append(agency_name)

# Create a DataFrame with the agency name included
df = pd.DataFrame({
    "Title": titles,
    "Date": dates,
    "Category": categories,
    "Link": links,
    "Agency": agencies
})

# Show the first few rows
print(df.head())


```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}



```

* c. Test Partner's Code (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

def scrape_enforcement_actions(year, month):
    # Step 1: Check if the year is valid (>= 2013)
    if year < 2013:
        print("Please restrict to a year >= 2013, as only enforcement actions from 2013 onwards are listed.")
        return None
    
    # Step 2: Format the year and month to be used in the URL
    year_month = f"{year}-{month:02d}"
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    
    # Lists to hold data
    titles, dates, categories, links, agencies = [], [], [], [], []
    
    # Initialize page number
    page_num = 1
    while True:
        # Step 3: Construct the URL for the current page
        url = f"{base_url}?page={page_num}"
        
        # Send a GET request to fetch the page content
        response = requests.get(url)
        
        if response.status_code != 200:
            print(f"Error fetching page {page_num}: {response.status_code}")
            break  # Exit the loop if page can't be fetched
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find enforcement action entries
        items = soup.select('h2.usa-card__heading')
        if not items:
            print("No items found, breaking the loop.")
            break  # No more items, break the loop
        
        print(f"Scraping page {page_num}...")
        
        # Extract data from each enforcement action
        for item in items:
            title = item.select_one('a').text.strip()
            link = "https://oig.hhs.gov" + item.select_one('a')['href']
            category = item.find_next('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1').text.strip()
            date = item.find_next('span', class_='text-base-dark padding-right-105').text.strip()
            
            # Visit the link to extract the agency
            enforcement_page = requests.get(link)
            if enforcement_page.status_code != 200:
                print(f"Error fetching {link}: {enforcement_page.status_code}")
                continue  # Skip this entry if the link fails
            
            enforcement_soup = BeautifulSoup(enforcement_page.text, 'html.parser')
            agency_tag = enforcement_soup.select('div.content p:nth-of-type(3)')
            agency_name = agency_tag[0].text.strip() if agency_tag else "Unknown"
            
            # Append to lists
            titles.append(title)
            links.append(link)
            categories.append(category)
            dates.append(date)
            agencies.append(agency_name)
        
        # Add a random delay between 1 to 3 seconds to prevent overloading the server
        time.sleep(random.uniform(1, 3))
        
        # Move to the next page
        page_num += 1
    
    # Step 4: Create a DataFrame with the collected data
    df = pd.DataFrame({
        "Title": titles,
        "Date": dates,
        "Category": categories,
        "Link": links,
        "Agency": agencies
    })
    
    # Step 5: Save the DataFrame to a .csv file
    file_name = f"enforcement_actions_{year}-{month:02d}.csv"
    df.to_csv(file_name, index=False)
    
    # Step 6: Return the DataFrame
    return df

# Example usage: scrape enforcement actions since January 2023
df = scrape_enforcement_actions(2023, 1)
if df is not None:
    print(f"Total enforcement actions scraped: {len(df)}")
    print(f"Earliest enforcement action details:\n{df.iloc[0]}")

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
import pandas as pd
import altair as alt

# Assuming the 'df' DataFrame is loaded with the scraped data
# Clean the 'Date' column to ensure it's in datetime format
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Step 1: Plot the Number of Enforcement Actions Over Time (aggregated by month and year)
df['Year_Month'] = df['Date'].dt.to_period('M')

# Aggregating data by Year-Month
enforcement_by_month = df.groupby('Year_Month').size().reset_index(name='Num_Actions')

# Plot using Altair
line_chart = alt.Chart(enforcement_by_month).mark_line().encode(
    x='Year_Month:T',
    y='Num_Actions:Q',
    tooltip=['Year_Month:T', 'Num_Actions:Q']
).properties(title='Number of Enforcement Actions Over Time')

line_chart.show()


# Step 2: Classify the actions into the "Criminal and Civil Actions" and "State Enforcement Agencies"
# We will manually classify topics by looking at keywords in the title

def classify_topic(title):
    title = title.lower()
    if 'health care' in title:
        return 'Health Care Fraud'
    elif 'financial' in title or 'bank' in title:
        return 'Financial Fraud'
    elif 'drug' in title:
        return 'Drug Enforcement'
    elif 'bribery' in title or 'corruption' in title:
        return 'Bribery/Corruption'
    else:
        return 'Other'

# Adding a 'Topic' column based on the title
df['Topic'] = df['Title'].apply(classify_topic)

# Step 3: Create the 'Criminal and Civil Actions' vs. 'State Enforcement Agencies' classification
# We'll assume that 'State Enforcement Agencies' can be determined based on the category or title

def classify_enforcement_type(category):
    if 'State' in category or 'State Enforcement' in category:
        return 'State Enforcement Agencies'
    else:
        return 'Criminal and Civil Actions'

df['Enforcement_Type'] = df['Category'].apply(classify_enforcement_type)

# Aggregating the data by Enforcement Type and Month
enforcement_by_type_month = df.groupby(['Year_Month', 'Enforcement_Type']).size().reset_index(name='Num_Actions')

# Plot using Altair: Split by Enforcement Type
enforcement_by_type_chart = alt.Chart(enforcement_by_type_month).mark_line().encode(
    x='Year_Month:T',
    y='Num_Actions:Q',
    color='Enforcement_Type:N',
    tooltip=['Year_Month:T', 'Num_Actions:Q', 'Enforcement_Type:N']
).properties(title='Enforcement Actions by Type (Criminal & Civil vs State Agencies)')

enforcement_by_type_chart.show()


# Step 4: Plot the Number of Enforcement Actions by Topic in "Criminal and Civil Actions"
# Filter the data for "Criminal and Civil Actions"
df_criminal_civil = df[df['Enforcement_Type'] == 'Criminal and Civil Actions']

# Aggregating data by Topic and Month
enforcement_by_topic = df_criminal_civil.groupby(['Year_Month', 'Topic']).size().reset_index(name='Num_Actions')

# Plot using Altair: Split by Topic
topic_chart = alt.Chart(enforcement_by_topic).mark_line().encode(
    x='Year_Month:T',
    y='Num_Actions:Q',
    color='Topic:N',
    tooltip=['Year_Month:T', 'Num_Actions:Q', 'Topic:N']
).properties(title='Enforcement Actions by Topic in Criminal and Civil Actions')

topic_chart.show()

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import pandas as pd
import geopandas as gpd
import altair as alt

# Clean the 'Agency' column to extract state names for state-level enforcement actions
def extract_state(agency_name):
    # Check if the agency is a state-level agency (contains "State of")
    if 'State of' in agency_name:
        # Extract the state name (after "State of")
        state_name = agency_name.split("State of")[-1].strip()
        return state_name
    else:
        return None

# Add a 'State' column to the dataframe for state-level agencies
df['State'] = df['Agency'].apply(extract_state)

# Filter the dataset to only include state-level enforcement actions
state_actions = df.dropna(subset=['State'])

# Load the shapefile for US states
states_shapefile = 'path_to_state_shapefile.shp'  # Replace with your actual path
states_gdf = gpd.read_file(states_shapefile)

# Merge the enforcement actions with the states shapefile on the 'State' column
merged_states = states_gdf.merge(state_actions.groupby('State').size().reset_index(name='Num_Actions'), 
                                 left_on='NAME', right_on='State', how='left')

# Plot the choropleth map
state_map = alt.Chart(merged_states).mark_geoshape().encode(
    color='Num_Actions:Q',
    tooltip=['NAME:N', 'Num_Actions:Q']
).properties(
    title='Number of Enforcement Actions by State (State-Level Agencies)',
    width=800,
    height=500
)

state_map.show()

```


### 2. Map by District (PARTNER 2)

```{python}
# Clean the 'Agency' column to extract district names for US Attorney District-level agencies
def extract_district(agency_name):
    # Check if the agency is a US Attorney District-level agency (contains "District")
    if 'District' in agency_name:
        # Extract the district name (after "District")
        district_name = agency_name.split("District")[-1].strip()
        return district_name
    else:
        return None

# Add a 'District' column to the dataframe for district-level agencies
df['District'] = df['Agency'].apply(extract_district)

# Filter the dataset to only include US Attorney District-level enforcement actions
district_actions = df.dropna(subset=['District'])

# Load the shapefile for US Attorney Districts (replace with actual shapefile path)
district_shapefile = 'path_to_district_shapefile.shp'  # Replace with your actual path
districts_gdf = gpd.read_file(district_shapefile)

# Merge the enforcement actions with the districts shapefile on the 'District' column
merged_districts = districts_gdf.merge(district_actions.groupby('District').size().reset_index(name='Num_Actions'), 
                                       left_on='district_name', right_on='District', how='left')

# Plot the choropleth map
district_map = alt.Chart(merged_districts).mark_geoshape().encode(
    color='Num_Actions:Q',
    tooltip=['district_name:N', 'Num_Actions:Q']
).properties(
    title='Number of Enforcement Actions by US Attorney District',
    width=800,
    height=500
)

district_map.show()

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```