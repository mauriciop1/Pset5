---
title: "PS5"
format: html
---

```{python}
!pip install lxml
!pip install beautifulsoup4 pandas
!pip install requests
```


```{python}
### 1. Scraping (PARTNER 1)

import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL of the HHS OIG Enforcement Actions page
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send a GET request to fetch the page content
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')

# Lists to hold data
titles, dates, categories, links = [], [], [], []

# Find enforcement action entries
for item in soup.select('h2.usa-card__heading'):
    # Extract title
    title = item.select_one('a').text.strip()
    
    # Extract and complete the link
    link = "https://oig.hhs.gov" + item.select_one('a')['href']
# Extract category
    category = item.find_next('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1').text.strip()
    
    # Extract date
    date = item.find_next('span', class_='text-base-dark padding-right-105').text.strip()
    
    # Append to lists
    titles.append(title)
    links.append(link)
    categories.append(category)
    dates.append(date)

# Create a DataFrame
df = pd.DataFrame({
    "Title": titles,
    "Date": dates,
    "Category": categories,
    "Link": links
})

# Show the first few rows
print(df.head())
```

```{python}
### 2. Crawling (PARTNER 1)

# Initialize an empty list for agency names
agencies = []

# Loop through each enforcement action link in the dataframe
for link in df['Link']:
    response = requests.get(link)  # Send request to the enforcement page
    soup = BeautifulSoup(response.text, 'lxml')  # Parse the page
    
    # Find the <h2> tag containing "Action Details"
    h2_tag = soup.find('h2', class_='font-heading-lg')
    
    # Initialize agency_name with a default value
    agency_name = "No agency section found"
    
    # If <h2> exists, get the following <ul> containing the agency data
    if h2_tag:
        ul_tag = h2_tag.find_next('ul', class_='usa-list--unstyled')
        
        if ul_tag:
            li_tags = ul_tag.find_all('li')
            
            # Check if there are at least two <li> tags
            if len(li_tags) >= 2:
                # Only split if "Agency:" is in the text to avoid index errors
                if "Agency:" in li_tags[1].text:
                    agency_name = li_tags[1].text.split('Agency:')[1].strip()
                else:
                    agency_name = "Agency name not found"
            else:
                agency_name = "No agency information available"
    
    # Append the agency name to the list
    agencies.append(agency_name)

# Add the agency names to the dataframe
df['Agency'] = agencies

# Print the updated dataframe
print(df.head())


```



## Step 2: Partner 2

### Part a. Turning the scraper into a function 

## a. Pseudo-Code 

1. Defining a function with Year and Month as key inputs

2. Validate Year:  
   If year < 2013, print a reminder and exit.

3. Set Base URL:  
   Define the URL 'https://oig.hhs.gov/fraud/enforcement/'.

4. Loop Through Pages:
    -
   - Start at page = 1.
   - If the year > 2013 continue until no more enforcement actions are found until the current date.

5. Scrape Data:  
   - Extract: 
• Date, Title of the enforcement action, Category (e.g, “Criminal and Civil Actions”), and link for each agency
   

6. Rate Limiting:
Include a time.sleep(1) pause after each page request to add a one-second delay between requests.

7. Data Storage:
Accumulate the scraped data, convert it into a DataFrame, and save it as a CSV file.

8. Return the DataFrame.

Step 2 Part b (Partner 2)

```{python}

## b. Create Dynamic Scraper (PARTNER 2)

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime

def scrape_enforcement_actions(year, month):
    # Check year validity
    if year < 2013:
        print("Please restrict the year to >= 2013.")
        return

    # Base URL and setup
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    page = 1
    all_data = []
    target_date = datetime(year, month, 1)
    
    while True:
        print(f"Scraping page {page}...")
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        
        # Check if the response is empty or redirected
        if response.status_code != 200:
            print(f"Failed to retrieve page {page}, status code: {response.status_code}")
            break
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Print a sample of the HTML for debugging
        print(soup.prettify()[:1000])  # Print the first 1000 characters of the HTML
        
        # Select all action cards
        action_cards = soup.select('.usa-card')
        
        # Debugging: check if the action cards are being found
        if not action_cards:
            print("No actions found on this page. Exiting loop.")
            break
        else:
            print(f"Found {len(action_cards)} actions on page {page}.")

        # Process each action card in one pass
        for card in action_cards:
            title = card.select_one('.usa-card__heading').get_text(strip=True) if card.select_one('.usa-card__heading') else "No title found"
            link_elem = card.select_one('.usa-card__heading a')
            link = link_elem['href'] if link_elem else "No link found"
            date_elem = card.select_one('.text-base-dark')
            date_text = date_elem.get_text(strip=True) if date_elem else "No date found"
            
            if date_text != "No date found":
                date = datetime.strptime(date_text, '%B %d, %Y')
                if date < target_date:
                    print(f"Reached actions from before {target_date.strftime('%B %Y')}. Stopping scrape.")
                    break
            
            # Initialize default values for category and agency
            category = "Unknown Category"
            agency = "No agency found"

            # Skip action page request if link not found
            if link != "No link found":
                response_action = requests.get(f"https://oig.hhs.gov{link}")
                action_soup = BeautifulSoup(response_action.text, 'html.parser')
                
                h2_tag = action_soup.find('h2', class_='font-heading-lg')
                if h2_tag:
                    ul_tag = h2_tag.find_next('ul', class_='usa-list--unstyled')
                    if ul_tag:
                        li_tags = ul_tag.find_all('li')
                        if len(li_tags) > 1 and 'Agency:' in li_tags[1].text:
                            agency = li_tags[1].text.split('Agency:')[1].strip()
                        if len(li_tags) > 2 and 'Enforcement Types:' in li_tags[2].text:
                            category = li_tags[2].text.split('Enforcement Types:')[1].strip()
            
            # Append the collected data
            all_data.append([title, date_text, category, link, agency])
        
        # Break the loop if we've hit the target date
        if date_text != "No date found" and date < target_date:
            break

        # Go to the next page and wait
        page += 1
        time.sleep(1)
    
    # Convert collected data into DataFrame and save if data exists
    if all_data:
        print(f"Collected {len(all_data)} enforcement actions in total.")
        df = pd.DataFrame(all_data, columns=["Title", "Date", "Category", "Link", "Agency"])
        filename = f"enforcement_actions_{year}_{month}.csv"
        df.to_csv(filename, index=False)
        print(f"Data saved to {filename}")
    else:
        print("No data was collected.")
    
    return df

# Run the function for debugging
df = scrape_enforcement_actions(2023, 1)
print(df.head())

```


## b. Create Dynamic Scraper (PARTNER 2.. continued)
Total enforcement actions collected: 1534
Earliest enforcement action details:
Title       Podiatrist Pays $90,000 To Settle False Billin...
Date                                      2023-01-03 00:00:00
Category                           Criminal and Civil Actions
Link        /fraud/enforcement/podiatrist-pays-90000-to-se...
Agency      U.S. Attorney’s Office, Southern District of T...
Name: 1533, dtype: object

```{python}
eval: False
# Run the function for data collection since January 2021
df = scrape_enforcement_actions(2021, 1)
print(df.head())
```

Step 3 Part1 (Partner 2)


```{python}
import pandas as pd
import altair as alt

# Load the CSV data into a DataFrame
df = pd.read_csv('C:/Users/arifm/OneDrive/Documents/GitHub/Pset5/enforcement_actions_2021_1.csv')


# Convert the 'Date' column to datetime format (assuming the 'Date' column is present in the CSV)
df['Date'] = pd.to_datetime(df['Date'])

# Extract Year and Month from 'Date' for grouping
df['YearMonth'] = df['Date'].dt.to_period('M')

# Aggregate the data by YearMonth to get the count of actions per month
monthly_counts = df.groupby('YearMonth').size().reset_index(name='Count')
monthly_counts['YearMonth'] = monthly_counts['YearMonth'].dt.to_timestamp()  # Convert to timestamp for Altair

# Create the line chart using Altair
line_chart = alt.Chart(monthly_counts).mark_line().encode(
    x=alt.X('YearMonth:T', title='Month-Year'),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions')
).properties(
    title="Number of Enforcement Actions Over Time (Monthly)",
    width=700,
    height=400
)
# Display the chart
line_chart.display()
```

Step 4. Part 2 (Partner 2)

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt 
# Step 1: Load the shapefile for US Attorney Districts
# Correct the quote mark at the end of the file path
US_Attorney_path_shp= "C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\Pset5\\geo_export_bb370b2c-46eb-4eb9-a2de-46b19e5bb7b1.shp"

# Read the shapefile
gdf_districts = gpd.read_file(US_Attorney_path_shp)

# Inspect the shapefile to confirm column names and district format
print("Shapefile columns:", gdf_districts.columns)
print(gdf_districts.head())

# Step 2: Filter enforcement actions to include only district-level records
district_df = df[df['Category'] == 'State Enforcement Agencies']
district_df = district_df[district_df['Agency'].str.contains('District', case=False, na=False)]

# Display filtered data to verify
print("Filtered district-level enforcement actions:")
print(district_df[['Agency', 'Category']].head())

# Step 3: Extract district name, standardize format, and count actions
district_df['District'] = district_df['Agency'].str.extract(r'(District.*)').fillna('')
district_df['District'] = district_df['District'].str.replace(r'[^a-zA-Z\s]', '', regex=True).str.strip()
district_counts = district_df.groupby('District').size().reset_index(name='Count')

# Display district_counts for validation
print("District counts DataFrame:")
print(district_counts.head())

# Step 4: Prepare shapefile by aligning district names for merging
# Standardize judicial_d in the GeoDataFrame for comparison
gdf_districts['judicial_d'] = gdf_districts['judicial_d'].str.replace(r'[^a-zA-Z\s]', '', regex=True).str.strip()

# Merge district_counts with shapefile GeoDataFrame
merged_gdf = gdf_districts.merge(district_counts, left_on='judicial_d', right_on='District', how='left')
merged_gdf['Count'] = merged_gdf['Count'].fillna(0)  # Fill NaNs with 0 for missing districts

# Verify that merged_gdf has non-zero counts where appropriate
print("Merged GeoDataFrame with Count values:")
print(merged_gdf[['judicial_d', 'Count']].head(20))

# Step 5: Plot the map using geopandas
fig, ax = plt.subplots(1, 1, figsize=(12, 8))
merged_gdf.plot(column='Count', cmap='YlGnBu', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title('US Attorney District-Level Enforcement Actions')
plt.axis('off')  # Optional: Turn off axis for a cleaner map

plt.show()
```